{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "## Harini Lakshmanan\n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "from lexical_diversity import lex_div as ld\n",
    "import csv\n",
    "import html\n",
    "import textacy.preprocessing as tprep\n",
    "import spacy\n",
    "import regex as re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e164b5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x17fbd12ff70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"C:\\\\Users\\\\gauth\\\\Documents\\\\Harini\\\\San Diego University\\\\applied text mining\\\\M1 Assignment Data\\\\M1 Results\\\\\"\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter\\\\\"\n",
    "lyrics_folder = \"lyrics\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, top_n_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = ld.ttr(tokens) # Simple TTR = len(Counter(text))/len(text)\n",
    "    num_characters = sum([len(i) for i in tokens])\n",
    "    \n",
    "    if verbose:        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        print(f\"The top {top_n_tokens} most common tokens\")\n",
    "        print(Counter(tokens).most_common(top_n_tokens))\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "The top 5 most common tokens\n",
      "[('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: Assertion statements are a way to make sure your code is working as you expect it to. You create examples that you know to be true and have the answer for. Then you run your code through your predefined examples to make sure it is running as expected.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5b8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dictionary Approach 1 - first\n",
    "\n",
    "# d[artist][title] = “the song lyrics as a string”\n",
    "# d = {}\n",
    "# # Get the directory location\n",
    "# directory = data_location + lyrics_folder\n",
    "# # Get all the subfolders in directory.\n",
    "# artist_subfolders = [name for name in os.listdir(directory) if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "\n",
    "# # Get all the files in each of the subfolders\n",
    "# for artist in artist_subfolders:\n",
    "#     d[artist] = {}\n",
    "#     for filename in os.listdir(directory + artist):\n",
    "#         f = os.path.join(directory + artist, filename)\n",
    "#         # checking if it is a file\n",
    "#         if os.path.isfile(f):\n",
    "#             with open(f) as file:\n",
    "#                 title = file.readline().strip()\n",
    "#                 d[artist][title] = file.read().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d70801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "# Dictionary Approach 2 - Using defaultdict\n",
    "\n",
    "# d[artist][title] = “the song lyrics as a string”\n",
    "lyrics = defaultdict(lambda: defaultdict(str))\n",
    "#  \n",
    "# Get the directory location\n",
    "directory = data_location + lyrics_folder\n",
    "# Get all the subfolders in directory.\n",
    "artist_subfolders = [name for name in os.listdir(directory) if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "\n",
    "# Get all the files in each of the subfolders\n",
    "for artist in artist_subfolders:\n",
    "    for filename in os.listdir(directory + artist):\n",
    "        f = os.path.join(directory + artist, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            with open(f) as file:\n",
    "                title = file.readline().strip()\n",
    "                lyrics[artist][title] = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14055f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"88 Degrees\"</td>\n",
       "      <td>Stuck in L.A., ain't got no friends \\nAnd so H...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"A Different Kind Of Love Song\"</td>\n",
       "      <td>What if the world was crazy and I was sane\\nWo...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"After All\"</td>\n",
       "      <td>Well, here we are again\\nI guess it must be fa...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Again\"</td>\n",
       "      <td>Again evening finds me at your door \\nHere to ...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Alfie\"</td>\n",
       "      <td>What's it all about, Alfie?\\nIs it just for th...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>\"We Dance To The Beat\"</td>\n",
       "      <td>We dance to the beat\\nWe dance to the beat\\nWe...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>\"Where Did Our Love Go\"</td>\n",
       "      <td>Thoughts about you and me \\nThinkin' about wha...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>\"Who's That Girl\"</td>\n",
       "      <td>Good girls are pretty like all the time\\nI'm j...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>\"With Every Heartbeat\"</td>\n",
       "      <td>Maybe we could make it all right\\nWe could mak...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>\"You've Got That Something\"</td>\n",
       "      <td>Look at me here I am\\nI'm givin all of my lovi...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>719 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0                       \"88 Degrees\"   \n",
       "1    \"A Different Kind Of Love Song\"   \n",
       "2                        \"After All\"   \n",
       "3                            \"Again\"   \n",
       "4                            \"Alfie\"   \n",
       "..                               ...   \n",
       "714           \"We Dance To The Beat\"   \n",
       "715          \"Where Did Our Love Go\"   \n",
       "716                \"Who's That Girl\"   \n",
       "717           \"With Every Heartbeat\"   \n",
       "718      \"You've Got That Something\"   \n",
       "\n",
       "                                                lyrics artist  \n",
       "0    Stuck in L.A., ain't got no friends \\nAnd so H...   cher  \n",
       "1    What if the world was crazy and I was sane\\nWo...   cher  \n",
       "2    Well, here we are again\\nI guess it must be fa...   cher  \n",
       "3    Again evening finds me at your door \\nHere to ...   cher  \n",
       "4    What's it all about, Alfie?\\nIs it just for th...   cher  \n",
       "..                                                 ...    ...  \n",
       "714  We dance to the beat\\nWe dance to the beat\\nWe...  robyn  \n",
       "715  Thoughts about you and me \\nThinkin' about wha...  robyn  \n",
       "716  Good girls are pretty like all the time\\nI'm j...  robyn  \n",
       "717  Maybe we could make it all right\\nWe could mak...  robyn  \n",
       "718  Look at me here I am\\nI'm givin all of my lovi...  robyn  \n",
       "\n",
       "[719 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create lyrics Pandas Dataframe for Cleaning\n",
    "artists = ['cher', 'robyn']\n",
    "\n",
    "for i, artist in enumerate(artists):\n",
    "    if i == 0:\n",
    "        lyrics_df = pd.DataFrame(lyrics[artist].items(), columns=['title', 'lyrics'])\n",
    "        lyrics_df['artist'] = artist\n",
    "    lyrics_dfi = pd.DataFrame(lyrics[artist].items(), columns=['title', 'lyrics'])\n",
    "    lyrics_dfi['artist'] = artist\n",
    "    lyrics_df = pd.concat([lyrics_df, lyrics_dfi], ignore_index=True)\n",
    "\n",
    "lyrics_df = lyrics_df.fillna(value='')\n",
    "lyrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e594ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the twitter data\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt / 10)\n",
    "\n",
    "desc = defaultdict(list)\n",
    "artists = ['cher', 'robyn']\n",
    "filename = ['cher_followers_data.txt', 'robynkonichiwa_followers_data.txt']\n",
    "directory = data_location + twitter_folder\n",
    "\n",
    "def remove_null_chars(text):\n",
    "    return text.replace('\\0', '')\n",
    "\n",
    "for i, artist in enumerate(artists):\n",
    "    f = os.path.join(directory, filename[i])\n",
    "\n",
    "    if os.path.isfile(f):\n",
    "        with open(f, 'rb') as f:  # Open in binary mode\n",
    "            reader = csv.DictReader((line.decode('utf-8', 'ignore').replace('\\0', '') for line in f), delimiter=\"\\t\")\n",
    "\n",
    "            for row in reader:\n",
    "                for (k, v) in row.items():\n",
    "                    if k == \"description\":\n",
    "                        desc[artist].append(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c761ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>csu</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184476</th>\n",
       "      <td>singer of songs, type 1 diabetic, tired $jakel...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184477</th>\n",
       "      <td>Dadx2/ Con-Arch/ Photographer/ DK #stemgrønnes...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184478</th>\n",
       "      <td>A year to change a life is still a year ✨😌</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184479</th>\n",
       "      <td>Head of Consumer - Mango. Made in Melbourne. R...</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184480</th>\n",
       "      <td>Stand for what is right, even if you stand alone.</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8184481 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               description artist\n",
       "0                                                            cher\n",
       "1                 𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜   cher\n",
       "2                163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡   cher\n",
       "3                                                      csu   cher\n",
       "4        Writer @Washinformer @SpelmanCollege alumna #D...   cher\n",
       "...                                                    ...    ...\n",
       "8184476  singer of songs, type 1 diabetic, tired $jakel...  robyn\n",
       "8184477  Dadx2/ Con-Arch/ Photographer/ DK #stemgrønnes...  robyn\n",
       "8184478         A year to change a life is still a year ✨😌  robyn\n",
       "8184479  Head of Consumer - Mango. Made in Melbourne. R...  robyn\n",
       "8184480  Stand for what is right, even if you stand alone.  robyn\n",
       "\n",
       "[8184481 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Pandas Dataframe for Cleaning\n",
    "for i, artist in enumerate(artists):\n",
    "    if i == 0:\n",
    "        twitter_df = pd.DataFrame.from_dict(desc[artist], orient='columns')\n",
    "        twitter_df.rename({0:\"description\"}, inplace=True,  axis=1)\n",
    "        twitter_df['artist'] = artist\n",
    "    twitter_dfi = pd.DataFrame.from_dict(desc[artist])\n",
    "    twitter_dfi.rename({0:\"description\"}, inplace=True,  axis=1)\n",
    "    twitter_dfi['artist'] = artist\n",
    "    twitter_df = pd.concat([twitter_df, twitter_dfi], ignore_index=True)\n",
    "\n",
    "\n",
    "twitter_df = twitter_df.fillna(value=\"\")\n",
    "twitter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7911682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data\n",
    "\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text)\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # Remove Puncuation\n",
    "    def remove_punc(s):\n",
    "        return ''.join(ch for ch in s if ch not in punctuation)\n",
    "    text = remove_punc(text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'[\\w-]*\\p{L}[\\w-]*', text)\n",
    "\n",
    "def remove_stop(tokens):\n",
    "    return [t for t in tokens if t.lower() not in sw]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b327033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>artist</th>\n",
       "      <th>clean_description</th>\n",
       "      <th>normalized_description</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>cher</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "      <td>cher</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "      <td>Proud supporter of messy buns leggings</td>\n",
       "      <td>[Proud, supporter, of, messy, buns, leggings]</td>\n",
       "      <td>[Proud, supporter, messy, buns, leggings]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>cher</td>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>163cm/愛かっふ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>[163cm, 愛かっふ, 26歳, 工〇好きな女の子, フォローしてくれたらDMします]</td>\n",
       "      <td>[163cm, 愛かっふ, 26歳, 工〇好きな女の子, フォローしてくれたらDMします]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>csu</td>\n",
       "      <td>cher</td>\n",
       "      <td>csu</td>\n",
       "      <td>csu</td>\n",
       "      <td>[csu]</td>\n",
       "      <td>[csu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "      <td>cher</td>\n",
       "      <td>Writer Washinformer SpelmanCollege alumna DCna...</td>\n",
       "      <td>Writer Washinformer SpelmanCollege alumna DCna...</td>\n",
       "      <td>[Writer, Washinformer, SpelmanCollege, alumna,...</td>\n",
       "      <td>[Writer, Washinformer, SpelmanCollege, alumna,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184476</th>\n",
       "      <td>singer of songs, type 1 diabetic, tired $jakel...</td>\n",
       "      <td>robyn</td>\n",
       "      <td>singer of songs type 1 diabetic tired jakelgil...</td>\n",
       "      <td>singer of songs type 1 diabetic tired jakelgil...</td>\n",
       "      <td>[singer, of, songs, type, diabetic, tired, jak...</td>\n",
       "      <td>[singer, songs, type, diabetic, tired, jakelgi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184477</th>\n",
       "      <td>Dadx2/ Con-Arch/ Photographer/ DK #stemgrønnes...</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Dadx2 ConArch Photographer DK stemgrønnest grø...</td>\n",
       "      <td>Dadx2 ConArch Photographer DK stemgrønnest grø...</td>\n",
       "      <td>[Dadx2, ConArch, Photographer, DK, stemgrønnes...</td>\n",
       "      <td>[Dadx2, ConArch, Photographer, DK, stemgrønnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184478</th>\n",
       "      <td>A year to change a life is still a year ✨😌</td>\n",
       "      <td>robyn</td>\n",
       "      <td>A year to change a life is still a year ✨😌</td>\n",
       "      <td>A year to change a life is still a year ✨😌</td>\n",
       "      <td>[A, year, to, change, a, life, is, still, a, y...</td>\n",
       "      <td>[year, change, life, still, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184479</th>\n",
       "      <td>Head of Consumer - Mango. Made in Melbourne. R...</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Head of Consumer Mango Made in Melbourne Rambl...</td>\n",
       "      <td>Head of Consumer Mango Made in Melbourne Rambl...</td>\n",
       "      <td>[Head, of, Consumer, Mango, Made, in, Melbourn...</td>\n",
       "      <td>[Head, Consumer, Mango, Made, Melbourne, Rambl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184480</th>\n",
       "      <td>Stand for what is right, even if you stand alone.</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Stand for what is right even if you stand alone</td>\n",
       "      <td>Stand for what is right even if you stand alone</td>\n",
       "      <td>[Stand, for, what, is, right, even, if, you, s...</td>\n",
       "      <td>[Stand, right, even, stand, alone]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8184481 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               description artist  \\\n",
       "0                                                            cher   \n",
       "1                 𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜   cher   \n",
       "2                163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡   cher   \n",
       "3                                                      csu   cher   \n",
       "4        Writer @Washinformer @SpelmanCollege alumna #D...   cher   \n",
       "...                                                    ...    ...   \n",
       "8184476  singer of songs, type 1 diabetic, tired $jakel...  robyn   \n",
       "8184477  Dadx2/ Con-Arch/ Photographer/ DK #stemgrønnes...  robyn   \n",
       "8184478         A year to change a life is still a year ✨😌  robyn   \n",
       "8184479  Head of Consumer - Mango. Made in Melbourne. R...  robyn   \n",
       "8184480  Stand for what is right, even if you stand alone.  robyn   \n",
       "\n",
       "                                         clean_description  \\\n",
       "0                                                            \n",
       "1                   𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜   \n",
       "2                163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡   \n",
       "3                                                      csu   \n",
       "4        Writer Washinformer SpelmanCollege alumna DCna...   \n",
       "...                                                    ...   \n",
       "8184476  singer of songs type 1 diabetic tired jakelgil...   \n",
       "8184477  Dadx2 ConArch Photographer DK stemgrønnest grø...   \n",
       "8184478         A year to change a life is still a year ✨😌   \n",
       "8184479  Head of Consumer Mango Made in Melbourne Rambl...   \n",
       "8184480    Stand for what is right even if you stand alone   \n",
       "\n",
       "                                    normalized_description  \\\n",
       "0                                                            \n",
       "1                   Proud supporter of messy buns leggings   \n",
       "2               163cm/愛かっふ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡   \n",
       "3                                                      csu   \n",
       "4        Writer Washinformer SpelmanCollege alumna DCna...   \n",
       "...                                                    ...   \n",
       "8184476  singer of songs type 1 diabetic tired jakelgil...   \n",
       "8184477  Dadx2 ConArch Photographer DK stemgrønnest grø...   \n",
       "8184478         A year to change a life is still a year ✨😌   \n",
       "8184479  Head of Consumer Mango Made in Melbourne Rambl...   \n",
       "8184480    Stand for what is right even if you stand alone   \n",
       "\n",
       "                                                 tokenized  \\\n",
       "0                                                       []   \n",
       "1            [Proud, supporter, of, messy, buns, leggings]   \n",
       "2            [163cm, 愛かっふ, 26歳, 工〇好きな女の子, フォローしてくれたらDMします]   \n",
       "3                                                    [csu]   \n",
       "4        [Writer, Washinformer, SpelmanCollege, alumna,...   \n",
       "...                                                    ...   \n",
       "8184476  [singer, of, songs, type, diabetic, tired, jak...   \n",
       "8184477  [Dadx2, ConArch, Photographer, DK, stemgrønnes...   \n",
       "8184478  [A, year, to, change, a, life, is, still, a, y...   \n",
       "8184479  [Head, of, Consumer, Mango, Made, in, Melbourn...   \n",
       "8184480  [Stand, for, what, is, right, even, if, you, s...   \n",
       "\n",
       "                                                    tokens  \n",
       "0                                                       []  \n",
       "1                [Proud, supporter, messy, buns, leggings]  \n",
       "2            [163cm, 愛かっふ, 26歳, 工〇好きな女の子, フォローしてくれたらDMします]  \n",
       "3                                                    [csu]  \n",
       "4        [Writer, Washinformer, SpelmanCollege, alumna,...  \n",
       "...                                                    ...  \n",
       "8184476  [singer, songs, type, diabetic, tired, jakelgi...  \n",
       "8184477  [Dadx2, ConArch, Photographer, DK, stemgrønnes...  \n",
       "8184478                  [year, change, life, still, year]  \n",
       "8184479  [Head, Consumer, Mango, Made, Melbourne, Rambl...  \n",
       "8184480                 [Stand, right, even, stand, alone]  \n",
       "\n",
       "[8184481 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean twitter data here\n",
    "twitter_df['clean_description'] = twitter_df['description'].map(clean)\n",
    "# Normalize Text\n",
    "twitter_df['normalized_description'] = twitter_df['clean_description'].map(normalize)\n",
    "# Tokenized Data\n",
    "twitter_df['tokenized'] = twitter_df['normalized_description'].map(tokenize)\n",
    "# Remove Stop Words\n",
    "twitter_df['tokens'] = twitter_df['tokenized'].map(remove_stop)\n",
    "\n",
    "twitter_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bbd4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data for Later Use\n",
    "twitter_df.to_pickle(\"twitter_df.pkl\")\n",
    "# twitter_df = pd.read_pickle(\"twitter_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>artist</th>\n",
       "      <th>clean_lyrics</th>\n",
       "      <th>normalized_lyrics</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"88 Degrees\"</td>\n",
       "      <td>Stuck in L.A., ain't got no friends \\nAnd so H...</td>\n",
       "      <td>cher</td>\n",
       "      <td>Stuck in LA aint got no friends And so Hollywo...</td>\n",
       "      <td>Stuck in LA aint got no friends And so Hollywo...</td>\n",
       "      <td>[Stuck, in, LA, aint, got, no, friends, And, s...</td>\n",
       "      <td>[Stuck, LA, aint, got, friends, Hollywood, nut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"A Different Kind Of Love Song\"</td>\n",
       "      <td>What if the world was crazy and I was sane\\nWo...</td>\n",
       "      <td>cher</td>\n",
       "      <td>What if the world was crazy and I was sane Wou...</td>\n",
       "      <td>What if the world was crazy and I was sane Wou...</td>\n",
       "      <td>[What, if, the, world, was, crazy, and, I, was...</td>\n",
       "      <td>[world, crazy, sane, Would, strange, cant, bel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"After All\"</td>\n",
       "      <td>Well, here we are again\\nI guess it must be fa...</td>\n",
       "      <td>cher</td>\n",
       "      <td>Well here we are again I guess it must be fate...</td>\n",
       "      <td>Well here we are again I guess it must be fate...</td>\n",
       "      <td>[Well, here, we, are, again, I, guess, it, mus...</td>\n",
       "      <td>[Well, guess, must, fate, Weve, tried, deep, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Again\"</td>\n",
       "      <td>Again evening finds me at your door \\nHere to ...</td>\n",
       "      <td>cher</td>\n",
       "      <td>Again evening finds me at your door Here to as...</td>\n",
       "      <td>Again evening finds me at your door Here to as...</td>\n",
       "      <td>[Again, evening, finds, me, at, your, door, He...</td>\n",
       "      <td>[evening, finds, door, ask, could, try, dont, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Alfie\"</td>\n",
       "      <td>What's it all about, Alfie?\\nIs it just for th...</td>\n",
       "      <td>cher</td>\n",
       "      <td>Whats it all about Alfie Is it just for the mo...</td>\n",
       "      <td>Whats it all about Alfie Is it just for the mo...</td>\n",
       "      <td>[Whats, it, all, about, Alfie, Is, it, just, f...</td>\n",
       "      <td>[Whats, Alfie, moment, live, Whats, sort, Alfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>\"We Dance To The Beat\"</td>\n",
       "      <td>We dance to the beat\\nWe dance to the beat\\nWe...</td>\n",
       "      <td>robyn</td>\n",
       "      <td>We dance to the beat We dance to the beat We d...</td>\n",
       "      <td>We dance to the beat We dance to the beat We d...</td>\n",
       "      <td>[We, dance, to, the, beat, We, dance, to, the,...</td>\n",
       "      <td>[dance, beat, dance, beat, dance, beat, dance,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>\"Where Did Our Love Go\"</td>\n",
       "      <td>Thoughts about you and me \\nThinkin' about wha...</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Thoughts about you and me Thinkin about what w...</td>\n",
       "      <td>Thoughts about you and me Thinkin about what w...</td>\n",
       "      <td>[Thoughts, about, you, and, me, Thinkin, about...</td>\n",
       "      <td>[Thoughts, Thinkin, used, Love, strong, one, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>\"Who's That Girl\"</td>\n",
       "      <td>Good girls are pretty like all the time\\nI'm j...</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Good girls are pretty like all the time Im jus...</td>\n",
       "      <td>Good girls are pretty like all the time Im jus...</td>\n",
       "      <td>[Good, girls, are, pretty, like, all, the, tim...</td>\n",
       "      <td>[Good, girls, pretty, like, time, Im, pretty, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>\"With Every Heartbeat\"</td>\n",
       "      <td>Maybe we could make it all right\\nWe could mak...</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Maybe we could make it all right We could make...</td>\n",
       "      <td>Maybe we could make it all right We could make...</td>\n",
       "      <td>[Maybe, we, could, make, it, all, right, We, c...</td>\n",
       "      <td>[Maybe, could, make, right, could, make, bette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>\"You've Got That Something\"</td>\n",
       "      <td>Look at me here I am\\nI'm givin all of my lovi...</td>\n",
       "      <td>robyn</td>\n",
       "      <td>Look at me here I am Im givin all of my lovin ...</td>\n",
       "      <td>Look at me here I am Im givin all of my lovin ...</td>\n",
       "      <td>[Look, at, me, here, I, am, Im, givin, all, of...</td>\n",
       "      <td>[Look, Im, givin, lovin, every, day, life, see...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>719 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "0                       \"88 Degrees\"   \n",
       "1    \"A Different Kind Of Love Song\"   \n",
       "2                        \"After All\"   \n",
       "3                            \"Again\"   \n",
       "4                            \"Alfie\"   \n",
       "..                               ...   \n",
       "714           \"We Dance To The Beat\"   \n",
       "715          \"Where Did Our Love Go\"   \n",
       "716                \"Who's That Girl\"   \n",
       "717           \"With Every Heartbeat\"   \n",
       "718      \"You've Got That Something\"   \n",
       "\n",
       "                                                lyrics artist  \\\n",
       "0    Stuck in L.A., ain't got no friends \\nAnd so H...   cher   \n",
       "1    What if the world was crazy and I was sane\\nWo...   cher   \n",
       "2    Well, here we are again\\nI guess it must be fa...   cher   \n",
       "3    Again evening finds me at your door \\nHere to ...   cher   \n",
       "4    What's it all about, Alfie?\\nIs it just for th...   cher   \n",
       "..                                                 ...    ...   \n",
       "714  We dance to the beat\\nWe dance to the beat\\nWe...  robyn   \n",
       "715  Thoughts about you and me \\nThinkin' about wha...  robyn   \n",
       "716  Good girls are pretty like all the time\\nI'm j...  robyn   \n",
       "717  Maybe we could make it all right\\nWe could mak...  robyn   \n",
       "718  Look at me here I am\\nI'm givin all of my lovi...  robyn   \n",
       "\n",
       "                                          clean_lyrics  \\\n",
       "0    Stuck in LA aint got no friends And so Hollywo...   \n",
       "1    What if the world was crazy and I was sane Wou...   \n",
       "2    Well here we are again I guess it must be fate...   \n",
       "3    Again evening finds me at your door Here to as...   \n",
       "4    Whats it all about Alfie Is it just for the mo...   \n",
       "..                                                 ...   \n",
       "714  We dance to the beat We dance to the beat We d...   \n",
       "715  Thoughts about you and me Thinkin about what w...   \n",
       "716  Good girls are pretty like all the time Im jus...   \n",
       "717  Maybe we could make it all right We could make...   \n",
       "718  Look at me here I am Im givin all of my lovin ...   \n",
       "\n",
       "                                     normalized_lyrics  \\\n",
       "0    Stuck in LA aint got no friends And so Hollywo...   \n",
       "1    What if the world was crazy and I was sane Wou...   \n",
       "2    Well here we are again I guess it must be fate...   \n",
       "3    Again evening finds me at your door Here to as...   \n",
       "4    Whats it all about Alfie Is it just for the mo...   \n",
       "..                                                 ...   \n",
       "714  We dance to the beat We dance to the beat We d...   \n",
       "715  Thoughts about you and me Thinkin about what w...   \n",
       "716  Good girls are pretty like all the time Im jus...   \n",
       "717  Maybe we could make it all right We could make...   \n",
       "718  Look at me here I am Im givin all of my lovin ...   \n",
       "\n",
       "                                             tokenized  \\\n",
       "0    [Stuck, in, LA, aint, got, no, friends, And, s...   \n",
       "1    [What, if, the, world, was, crazy, and, I, was...   \n",
       "2    [Well, here, we, are, again, I, guess, it, mus...   \n",
       "3    [Again, evening, finds, me, at, your, door, He...   \n",
       "4    [Whats, it, all, about, Alfie, Is, it, just, f...   \n",
       "..                                                 ...   \n",
       "714  [We, dance, to, the, beat, We, dance, to, the,...   \n",
       "715  [Thoughts, about, you, and, me, Thinkin, about...   \n",
       "716  [Good, girls, are, pretty, like, all, the, tim...   \n",
       "717  [Maybe, we, could, make, it, all, right, We, c...   \n",
       "718  [Look, at, me, here, I, am, Im, givin, all, of...   \n",
       "\n",
       "                                                tokens  \n",
       "0    [Stuck, LA, aint, got, friends, Hollywood, nut...  \n",
       "1    [world, crazy, sane, Would, strange, cant, bel...  \n",
       "2    [Well, guess, must, fate, Weve, tried, deep, i...  \n",
       "3    [evening, finds, door, ask, could, try, dont, ...  \n",
       "4    [Whats, Alfie, moment, live, Whats, sort, Alfi...  \n",
       "..                                                 ...  \n",
       "714  [dance, beat, dance, beat, dance, beat, dance,...  \n",
       "715  [Thoughts, Thinkin, used, Love, strong, one, d...  \n",
       "716  [Good, girls, pretty, like, time, Im, pretty, ...  \n",
       "717  [Maybe, could, make, right, could, make, bette...  \n",
       "718  [Look, Im, givin, lovin, every, day, life, see...  \n",
       "\n",
       "[719 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean lyrics data here\n",
    "lyrics_df['clean_lyrics'] = lyrics_df['lyrics'].map(clean)\n",
    "# Normalize Text\n",
    "lyrics_df['normalized_lyrics'] = lyrics_df['clean_lyrics'] .map(normalize)\n",
    "# Tokenized Data\n",
    "lyrics_df['tokenized'] = lyrics_df['normalized_lyrics'].map(tokenize)\n",
    "# Remove Stop Words\n",
    "lyrics_df['tokens'] = lyrics_df['tokenized'].map(remove_stop)\n",
    "\n",
    "lyrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99ed41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data for Later Use\n",
    "lyrics_df.to_pickle(\"lyrics_df.pkl\")\n",
    "# lyrics_df = pd.read_pickle(\"lyrics_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lyrics Data for Cher:\n",
      "\n",
      "There are 69548 tokens in the data.\n",
      "There are 4334 unique tokens in the data.\n",
      "There are 333830 characters in the data.\n",
      "The lexical diversity is 0.062 in the data.\n",
      "The top 5 most common tokens\n",
      "[('love', 1622), ('Im', 1020), ('know', 942), ('dont', 626), ('see', 568)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[69548, 4334, 0.062316673376660726, 333830]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls to descriptive_stats here\n",
    "\n",
    "# Helper Function\n",
    "def flatten_and_descriptive_stats(list_of_lists):\n",
    "    wordlist = [i for s in list_of_lists for i in s]\n",
    "    return descriptive_stats(wordlist)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLyrics Data for Cher:\\n\")\n",
    "flatten_and_descriptive_stats(lyrics_df.loc[lyrics_df['artist']=='cher'][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10ab0261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lyrics Data for Robyn:\n",
      "\n",
      "There are 12931 tokens in the data.\n",
      "There are 2409 unique tokens in the data.\n",
      "There are 62136 characters in the data.\n",
      "The lexical diversity is 0.186 in the data.\n",
      "The top 5 most common tokens\n",
      "[('Im', 254), ('know', 235), ('love', 214), ('got', 207), ('dont', 189)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12931, 2409, 0.1862964967906581, 62136]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLyrics Data for Robyn:\\n\")\n",
    "flatten_and_descriptive_stats(lyrics_df[lyrics_df['artist']=='robyn'][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f67e09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter Description Data for Cher:\n",
      "\n",
      "There are 30077144 tokens in the data.\n",
      "There are 1476381 unique tokens in the data.\n",
      "There are 183717856 characters in the data.\n",
      "The lexical diversity is 0.049 in the data.\n",
      "The top 5 most common tokens\n",
      "[('love', 300980), ('Im', 215364), ('life', 198284), ('de', 138336), ('Love', 111376)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[30077144, 1476381, 0.04908647576378927, 183717856]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTwitter Description Data for Cher:\\n\")\n",
    "flatten_and_descriptive_stats(twitter_df.loc[twitter_df['artist']=='cher'][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee533627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter Description Data for Robyn:\n",
      "\n",
      "There are 1443350 tokens in the data.\n",
      "There are 275032 unique tokens in the data.\n",
      "There are 9050822 characters in the data.\n",
      "The lexical diversity is 0.191 in the data.\n",
      "The top 5 most common tokens\n",
      "[('music', 9293), ('love', 8118), ('och', 7774), ('Im', 6992), ('de', 6075)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1443350, 275032, 0.1905511483701112, 9050822]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTwitter Description Data for Robyn:\\n\")\n",
    "flatten_and_descriptive_stats(twitter_df.loc[twitter_df['artist']=='robyn'][\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: Most definatly the top 5 words would have been things like \"a\", \"we\", \"here\", and \"the\".\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: Because of the lexical richness, I would have thought that the Cher's lyrics would be simpler than Robyn's. Cher's songs that I am familiar with frequently repeat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "<!-- What are the ten most common emojis by artist in the twitter descriptions?  -->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "269cd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Tokenized emoji Data\n",
    "twitter_df['emoji'] = twitter_df['description'].map(emoji.distinct_emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4087e7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter emoji Data for Robyn:\n",
      "\n",
      "There are 64530 tokens in the data.\n",
      "There are 2379 unique tokens in the data.\n",
      "There are 98878 characters in the data.\n",
      "The lexical diversity is 0.037 in the data.\n",
      "The top 10 most common tokens\n",
      "[('🏳️\\u200d🌈', 3071), ('❤️', 2088), ('♥', 1544), ('✨', 1510), ('🌈', 1265), ('❤', 1108), ('💙', 679), ('🎶', 623), ('💜', 622), ('🇺🇸', 572)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[64530, 2379, 0.03686657368665737, 98878]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper Function\n",
    "def flatten_and_descriptive_emoji_stats(list_of_lists):\n",
    "    wordlist = [i for s in list_of_lists for i in s]\n",
    "    return descriptive_stats(wordlist, 10)\n",
    "\n",
    "print(\"\\nTwitter emoji Data for Robyn:\\n\")\n",
    "flatten_and_descriptive_emoji_stats(twitter_df.loc[twitter_df['artist']=='robyn'][\"emoji\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16bb2f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter emoji Data for Cher:\n",
      "\n",
      "There are 1882150 tokens in the data.\n",
      "There are 3398 unique tokens in the data.\n",
      "There are 2696452 characters in the data.\n",
      "The lexical diversity is 0.002 in the data.\n",
      "The top 10 most common tokens\n",
      "[('❤️', 68448), ('🏳️\\u200d🌈', 56630), ('❤', 41720), ('✨', 40764), ('💙', 32796), ('♥', 30076), ('🌈', 29848), ('💜', 25652), ('🇺🇸', 24080), ('🌊', 20878)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1882150, 3398, 0.0018053821427622665, 2696452]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTwitter emoji Data for Cher:\\n\")\n",
    "flatten_and_descriptive_emoji_stats(twitter_df.loc[twitter_df['artist']=='cher'][\"emoji\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Helper function\n",
    "def hashtag_tokenizer(text):\n",
    "    return re.findall(r\"#(\\w+)\", text)\n",
    "    \n",
    "# Tokenized Hashtag Data\n",
    "twitter_df['hashtags'] = twitter_df['description'].map(hashtag_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64e0eccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter emoji Data for Cher:\n",
      "\n",
      "There are 857804 tokens in the data.\n",
      "There are 164866 unique tokens in the data.\n",
      "There are 7983378 characters in the data.\n",
      "The lexical diversity is 0.192 in the data.\n",
      "The top 10 most common tokens\n",
      "[('BLM', 19082), ('Resist', 12070), ('BlackLivesMatter', 9366), ('resist', 7594), ('FBR', 6480), ('TheResistance', 5988), ('blacklivesmatter', 5290), ('1', 5266), ('Resistance', 3834), ('RESIST', 3648)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[857804, 164866, 0.19219541993275854, 7983378]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTwitter emoji Data for Cher:\\n\")\n",
    "flatten_and_descriptive_emoji_stats(twitter_df.loc[twitter_df['artist']=='cher'][\"hashtags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3111cb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twitter emoji Data for Robyn:\n",
      "\n",
      "There are 30133 tokens in the data.\n",
      "There are 19884 unique tokens in the data.\n",
      "There are 270529 characters in the data.\n",
      "The lexical diversity is 0.660 in the data.\n",
      "The top 10 most common tokens\n",
      "[('BlackLivesMatter', 337), ('BLM', 307), ('blacklivesmatter', 208), ('1', 199), ('music', 174), ('Music', 113), ('EDM', 86), ('LGBTQ', 75), ('TeamFollowBack', 59), ('blm', 56)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[30133, 19884, 0.6598745561344705, 270529]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTwitter emoji Data for Robyn:\\n\")\n",
    "flatten_and_descriptive_emoji_stats(twitter_df.loc[twitter_df['artist']=='robyn'][\"hashtags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def title_clean_tokenize(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text)\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # Remove Puncuation\n",
    "    def remove_punc(s):\n",
    "        return ''.join(ch for ch in s if ch not in punctuation)\n",
    "    text = remove_punc(text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return re.findall(r'[\\w-]*\\p{L}[\\w-]*', text)\n",
    "\n",
    "\n",
    "# Tokenized Title Data\n",
    "lyrics_df['clean_title'] = lyrics_df['title'].map(title_clean_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cb274df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title Data for Robyn:\n",
      "\n",
      "There are 256 tokens in the data.\n",
      "There are 175 unique tokens in the data.\n",
      "There are 1159 characters in the data.\n",
      "The lexical diversity is 0.684 in the data.\n",
      "The top 5 most common tokens\n",
      "[('Me', 9), ('You', 8), ('The', 7), ('My', 6), ('Love', 5)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[256, 175, 0.68359375, 1159]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTitle Data for Robyn:\\n\")\n",
    "flatten_and_descriptive_stats(lyrics_df.loc[lyrics_df['artist']=='robyn'][\"clean_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "303ee47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title Data for Cher:\n",
      "\n",
      "There are 2310 tokens in the data.\n",
      "There are 510 unique tokens in the data.\n",
      "There are 9276 characters in the data.\n",
      "The lexical diversity is 0.221 in the data.\n",
      "The top 5 most common tokens\n",
      "[('The', 108), ('You', 82), ('Love', 72), ('I', 64), ('To', 56)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2310, 510, 0.22077922077922077, 9276]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTitle Data for Cher:\\n\")\n",
    "flatten_and_descriptive_stats(lyrics_df.loc[lyrics_df['artist']=='cher'][\"clean_title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Artist 1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Artist 2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: This will match on white space (spaces between words, tabs) and the + sign indicates that eithe one or more should be matched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not necessary since I already did all this earlier.\n",
    "# collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "# def tokenize_lyrics(lyric) : \n",
    "#     \"\"\"strip and split on whitespace\"\"\"\n",
    "#     return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2294c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your lyric length comparison chart here. \n",
    "lyrics_df['length'] = lyrics_df['tokens'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1831f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "cher     AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "robyn    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lyrics_df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3decbd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "22b6aa23ebdb95c31771a605332d65b96d6e70a70a8e5cf021766a53e709a3c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
